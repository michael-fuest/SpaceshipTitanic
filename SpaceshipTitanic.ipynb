{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import boruta as br\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in test and train datasets\n",
    "filepath = './data/'\n",
    "df_train = pd.read_csv(filepath + 'train.csv')\n",
    "df_test = pd.read_csv(filepath + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up pre-processing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import set_config\n",
    "\n",
    "#Telling sklearn that we want to output dataframes\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "#Defining numerical and categorical columns in df_train\n",
    "cat_vars = ['HomePlanet', 'Destination', 'Cabin', 'VIP', 'Age']\n",
    "num_vars = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'CryoSleep']\n",
    "#Note: Age included in categorical since it is bucketized later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.HomePlanet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking label balance\n",
    "df_train.loc[df_train.HomePlanet == 'Earth'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CatVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''Creates new categorical variables.\n",
    "            starboard: whether the cabin is on the starboard side of the ship\n",
    "            deck: the deck the passenger is on\n",
    "        '''\n",
    "        X['Starboard'] = X.apply(lambda x: 1 if x['Cabin'][-1] == 'S' else 0, axis=1)\n",
    "        X['Deck'] = X['Cabin'].str[0]\n",
    "        return X\n",
    "    \n",
    "class NumVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''Creates new numerical variables.\n",
    "            roommates: the number of roommates the passenger has\n",
    "            Crew: if the passenger is a passenger or a crew member, if spending on all services is 0 and passenger is not in cryosleep\n",
    "        '''\n",
    "        X['roommates'] = self.columns.map(self.columns.value_counts())\n",
    "        X['Crew'] = X.apply(lambda x: 1 if x['RoomService'] == 0 and x['FoodCourt'] == 0 and x['ShoppingMall'] == 0 and x['Spa'] == 0 and x['VRDeck'] == 0 and x['CryoSleep'] == 0 else 0, axis=1)\n",
    "        return X\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Drops specified columns.\n",
    "            columns: the columns to drop\n",
    "        '''\n",
    "        X.drop(self.columns, axis=1, inplace=True)\n",
    "        return X\n",
    "    \n",
    "class Bucketizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, bins, labels):\n",
    "        self.column_name = column_name\n",
    "        self.bins = bins\n",
    "        self.labels = labels\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Bucketizes the specified column.\n",
    "            column_name: the name of the column to bucketize\n",
    "            bins: the bins to use\n",
    "            labels: the labels to use\n",
    "        '''\n",
    "        X[self.column_name] = pd.cut(X[self.column_name], bins=self.bins, labels=self.labels)\n",
    "        return X\n",
    "    \n",
    "class SmartImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, condition_name, condition_value, column_names, value):\n",
    "        self.condition_name = condition_name\n",
    "        self.condition_value = condition_value\n",
    "        self.column_names = column_names\n",
    "        self.value = value\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Imputes values based on a condition.\n",
    "            condition_name: the name of the column to check\n",
    "            condition_value: the value of the column to check\n",
    "            column_names: the columns to impute\n",
    "            value: the value to impute\n",
    "        '''\n",
    "        for col in self.column_names:\n",
    "            X.loc[(X[self.condition_name] == self.condition_value), col] = self.value\n",
    "        return X\n",
    "    \n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_names, threshold):\n",
    "        self.column_names = column_names\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Winsorizes the specified column at a percentile threshold.\n",
    "            column_names: the names of the columns to winsorize\n",
    "            threshold: the upper percentile threshold to winsorize at (e.g. 0.05 will winsorize at the 95th percentiles)\n",
    "        '''\n",
    "        for col in self.column_names:\n",
    "            X[col].clip(upper = X[col].quantile(1 - self.threshold), axis=0, inplace=True).copy()\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the data pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('new_variables_adder', NumVariablesAdder(df_train['Cabin'])),\n",
    "    ('smart_imputer', SmartImputer('CryoSleep', True, ['Spa','RoomService', 'VRDeck', 'FoodCourt', 'ShoppingMall'], 0)),\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('outlier_remover', OutlierRemover([['Spa','RoomService', 'VRDeck', 'FoodCourt', 'ShoppingMall']], 0.05)),\n",
    "    ('std_scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('bucketizer', Bucketizer('Age', bins=[0, 10, 25, 75, 100], labels=['child', 'young adult', 'adult', 'senior'])),\n",
    "    ('smart_imputer_1', SmartImputer('HomePlanet', 'Earth', ['VIP'], False)),\n",
    "    ('smart_imputer_2', SmartImputer('VIP', True, ['Age'], 'adult')),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('new_variables_adder', CatVariablesAdder()),\n",
    "    ('column dropper', ColumnDropper('Cabin')),\n",
    "    ('one_hot_encoder', OneHotEncoder(sparse_output = False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding data pipelines to Column Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "        ([\n",
    "            ('numerical', num_pipeline, num_vars),\n",
    "            ('categorical', cat_pipeline, cat_vars)\n",
    "        ]), \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "#Transforming the data\n",
    "df_train_processed = data_pipeline.fit_transform(df_train)\n",
    "df_test_processed = data_pipeline.fit_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding modeling to pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_model_pipeline = Pipeline([\n",
    "    ('preprocessor', data_pipeline),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "gb_model_pipeline = Pipeline([\n",
    "    ('preprocessor', data_pipeline),\n",
    "    ('model', GradientBoostingClassifier(n_estimators=100,\n",
    "                                            max_depth=10,\n",
    "                                            learning_rate=0.1,\n",
    "                                            random_state=123,\n",
    "                                            verbose=0))\n",
    "])\n",
    "\n",
    "#Defining train and test splits for preprocessed train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train, df_train['Transported'], test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining grid parameters for random forest\n",
    "rf_grid_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "#Defining grid parameters for gradient boosting\n",
    "gb_grid_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    'model__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "#Defining grid search for random forest\n",
    "rf_grid_search = GridSearchCV(rf_model_pipeline, rf_grid_params, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "rf_grid_search.fit(x_train, y_train)\n",
    "\n",
    "#Defining grid search for gradient boosting\n",
    "gb_grid_search = GridSearchCV(gb_model_pipeline, gb_grid_params, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "gb_grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#Predicting on test data\n",
    "rf_pred = rf_grid_search.predict(x_test)\n",
    "gb_pred = gb_grid_search.predict(x_test)\n",
    "\n",
    "#Evaluating the models\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Random Forest Accuracy: ', accuracy_score(y_test, rf_pred))\n",
    "print('Gradient Boosting Accuracy: ', accuracy_score(y_test, gb_pred))\n",
    "\n",
    "print('Random Forest Classification Report: ', classification_report(y_test, rf_pred))\n",
    "print('Gradient Boosting Classification Report: ', classification_report(y_test, gb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting unseen data\n",
    "final_predictions = gb_grid_search.best_estimator_.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating submission file\n",
    "final_predictions = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': final_predictions})\n",
    "final_predictions.to_csv(filepath + 'submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:13:51) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19348e875bf2b969a1a986b4a58ef26d80a74b442efcafffa90e1d97deab2d1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
